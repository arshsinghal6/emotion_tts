{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'Tacotron2'\n",
    "CPU_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ESDDataset import ESDDataset\n",
    "\n",
    "d = ESDDataset(dataset_dir='./data/Emotion Speech Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.df[\"mel_text_pair\"] = d.df['audio_file_path'] + '|' + d.df['transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(d.df, test_size=0.2)\n",
    "\n",
    "train.to_csv(path_or_buf='./data/train_mel_text_pairs.txt', columns=['mel_text_pair'], index=False, header=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "test.to_csv(path_or_buf='./data/test_mel_text_pairs.txt', columns=['mel_text_pair'], index=False, header=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from train import parse_args\n",
    "from models import model_parser\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "parser = parse_args(args)\n",
    "parser = model_parser(MODEL_NAME, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args('--epochs 2 -lr 1e-4 -bs 2 -m Tacotron2 -o ./ --sampling-rate 16000'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.tacotron2.data_function import TextMelLoader\n",
    "\n",
    "train_dataset = TextMelLoader(dataset_path='', audiopaths_and_text='./data/train_mel_text_pairs.txt', args=args)\n",
    "val_dataset = TextMelLoader(dataset_path='', audiopaths_and_text='./data/test_mel_text_pairs.txt', args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import data_functions\n",
    "\n",
    "collate_fn = data_functions.get_collate_function(\n",
    "        MODEL_NAME, args.n_frames_per_step)\n",
    "train_loader = DataLoader(train_dataset, num_workers=1, shuffle=False,\n",
    "                              sampler=None,\n",
    "                              batch_size=args.batch_size, pin_memory=False,\n",
    "                              drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 185])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(iteration, epoch, optimizer, learning_rate,\n",
    "                         anneal_steps, anneal_factor, rank):\n",
    "\n",
    "    p = 0\n",
    "    if anneal_steps is not None:\n",
    "        for i, a_step in enumerate(anneal_steps):\n",
    "            if epoch >= int(a_step):\n",
    "                p = p+1\n",
    "\n",
    "    if anneal_factor == 0.3:\n",
    "        lr = learning_rate*((0.1 ** (p//2))*(1.0 if p % 2 == 0 else 0.3))\n",
    "    else:\n",
    "        lr = learning_rate*(anneal_factor ** p)\n",
    "\n",
    "    if optimizer.param_groups[0]['lr'] != lr:\n",
    "        print(step=(epoch, iteration), data={'learning_rate changed': str(optimizer.param_groups[0]['lr'])+\" -> \"+str(lr)})\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, valset, epoch, batch_iter, batch_size,\n",
    "             world_size, collate_fn, distributed_run, perf_bench, batch_to_gpu, amp_run):\n",
    "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loader = DataLoader(valset, num_workers=1, shuffle=False,\n",
    "                                sampler=None,\n",
    "                                batch_size=batch_size, pin_memory=False,\n",
    "                                collate_fn=collate_fn,\n",
    "                                drop_last=(True if perf_bench else False))\n",
    "\n",
    "        val_loss = 0.0\n",
    "        num_iters = 0\n",
    "        val_items_per_sec = 0.0\n",
    "        for i, batch in enumerate(val_loader):\n",
    "\n",
    "            x, y, num_items = batch_to_gpu(batch)\n",
    "            #AMP upstream autocast\n",
    "            with torch.cuda.amp.autocast(enabled=amp_run):\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "            reduced_val_loss = loss.item()\n",
    "            reduced_num_items = num_items.item()\n",
    "            val_loss += reduced_val_loss\n",
    "            \n",
    "            num_iters += 1\n",
    "\n",
    "        val_loss = val_loss/num_iters\n",
    "        val_items_per_sec = val_items_per_sec/num_iters\n",
    "\n",
    "\n",
    "        print(step=(epoch,), data={'val_loss': val_loss})\n",
    "        print(step=(epoch,), data={'val_items_per_sec': val_items_per_sec})\n",
    "\n",
    "        return val_loss, val_items_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scaler, epoch, config, output_dir,\n",
    "                    model_name, local_rank, world_size):\n",
    "\n",
    "    random_rng_state = torch.random.get_rng_state().cuda()\n",
    "    cuda_rng_state = torch.cuda.get_rng_state(local_rank).cuda()\n",
    "\n",
    "    random_rng_states_all = [torch.empty_like(random_rng_state) for _ in range(world_size)]\n",
    "    cuda_rng_states_all = [torch.empty_like(cuda_rng_state) for _ in range(world_size)]\n",
    "\n",
    "    # if world_size > 1:\n",
    "    #     dist.all_gather(random_rng_states_all, random_rng_state)\n",
    "    #     dist.all_gather(cuda_rng_states_all, cuda_rng_state)\n",
    "    # else:\n",
    "    random_rng_states_all = [random_rng_state]\n",
    "    cuda_rng_states_all = [cuda_rng_state]\n",
    "\n",
    "    random_rng_states_all = torch.stack(random_rng_states_all).cpu()\n",
    "    cuda_rng_states_all = torch.stack(cuda_rng_states_all).cpu()\n",
    "\n",
    "    if local_rank == 0:\n",
    "        checkpoint = {'epoch': epoch,\n",
    "                      'cuda_rng_state_all': cuda_rng_states_all,\n",
    "                      'random_rng_states_all': random_rng_states_all,\n",
    "                      'config': config,\n",
    "                      'state_dict': model.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict(),\n",
    "                      'scaler': scaler.state_dict()}\n",
    "\n",
    "        checkpoint_filename = \"checkpoint_{}_{}.pt\".format(model_name, epoch)\n",
    "        checkpoint_path = os.path.join(output_dir, checkpoint_filename)\n",
    "        print(\"Saving model and optimizer state at epoch {} to {}\".format(\n",
    "            epoch, checkpoint_path))\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "        symlink_src = checkpoint_filename\n",
    "        symlink_dst = os.path.join(\n",
    "            output_dir, \"checkpoint_{}_last.pt\".format(model_name))\n",
    "        if os.path.exists(symlink_dst) and os.path.islink(symlink_dst):\n",
    "            print(\"Updating symlink\", symlink_dst, \"to point to\", symlink_src)\n",
    "            os.remove(symlink_dst)\n",
    "\n",
    "        os.symlink(symlink_src, symlink_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import loss_functions\n",
    "\n",
    "model_config = models.get_model_config(MODEL_NAME, args)\n",
    "model = models.get_model(MODEL_NAME, model_config, CPU_RUN,\n",
    "                         uniform_initialize_bn_weight=not args.disable_uniform_initialize_bn_weight)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
    "\n",
    "criterion = loss_functions.get_loss_function(MODEL_NAME, sigma=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # valset = data_functions.get_data_loader(\n",
    "    #         MODEL_NAME, args.dataset_path, args.validation_files, args)\n",
    "    batch_to_gpu = data_functions.get_batch_to_gpu(MODEL_NAME)\n",
    "\n",
    "    local_rank, world_size = args.rank, args.world_size\n",
    "    distributed_run = world_size > 1\n",
    "\n",
    "    epoch = [0]\n",
    "    start_epoch = epoch[0]\n",
    "    iteration = 0\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        # used to calculate avg items/sec over epoch\n",
    "        reduced_num_items_epoch = 0\n",
    "        \n",
    "        num_iters = 0\n",
    "        reduced_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            adjust_learning_rate(iteration, epoch, optimizer, args.learning_rate,\n",
    "                                        args.anneal_steps, args.anneal_factor, local_rank)\n",
    "\n",
    "            model.zero_grad()\n",
    "            x, y, num_items = batch_to_gpu(batch)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            reduced_loss = loss.item()\n",
    "            reduced_num_items = num_items.item()\n",
    "            if np.isnan(reduced_loss):\n",
    "                raise Exception(\"loss is NaN\")\n",
    "            \n",
    "\n",
    "            print(f\"step={(epoch,i)}\", f\"data='train_loss':{reduced_loss}\")\n",
    "            num_iters += 1\n",
    "\n",
    "            # accumulate number of items processed in this epoch\n",
    "            reduced_num_items_epoch += reduced_num_items\n",
    "\n",
    "            if args.amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), args.grad_clip_thresh)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), args.grad_clip_thresh)\n",
    "                optimizer.step()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            \n",
    "            print(f\"step={(epoch,i)}\", f\"data=train_loss: {reduced_loss}\")\n",
    "            # print(step=(epoch,), data={'train_epoch_time': epoch_time})\n",
    "\n",
    "            # val_loss, val_items_per_sec = validate(model, criterion, val_dataset, epoch,\n",
    "            #                                     iteration, args.batch_size,\n",
    "            #                                     world_size, collate_fn,\n",
    "            #                                     distributed_run, args.bench_class==\"perf-train\",\n",
    "            #                                     batch_to_gpu,\n",
    "            #                                     args.amp)\n",
    "\n",
    "            # if (epoch % args.epochs_per_checkpoint == 0) and (args.bench_class == \"\" or args.bench_class == \"train\"):\n",
    "            #     save_checkpoint(model, optimizer, scaler, epoch, model_config,\n",
    "            #                     args.output, args.model_name, local_rank, world_size)\n",
    "                \n",
    "            # print(f\"step=tuple({(epoch,i)})\", f\"data=val_loss: {val_loss}\")\n",
    "            # print(f\"step=tuple({(epoch,i)})\", f\"data=train_loss:{reduced_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=(0, 0) data='train_loss':88.56124114990234\n",
      "step=(0, 0) data=train_loss: 88.56124114990234\n",
      "step=(0, 1) data='train_loss':72.9557876586914\n",
      "step=(0, 1) data=train_loss: 72.9557876586914\n",
      "step=(0, 2) data='train_loss':81.89559173583984\n",
      "step=(0, 2) data=train_loss: 81.89559173583984\n",
      "step=(0, 3) data='train_loss':74.14900207519531\n",
      "step=(0, 3) data=train_loss: 74.14900207519531\n",
      "step=(0, 4) data='train_loss':93.43402099609375\n",
      "step=(0, 4) data=train_loss: 93.43402099609375\n",
      "step=(0, 5) data='train_loss':77.14569854736328\n",
      "step=(0, 5) data=train_loss: 77.14569854736328\n",
      "step=(0, 6) data='train_loss':69.26527404785156\n",
      "step=(0, 6) data=train_loss: 69.26527404785156\n",
      "step=(0, 7) data='train_loss':90.31982421875\n",
      "step=(0, 7) data=train_loss: 90.31982421875\n",
      "step=(0, 8) data='train_loss':64.65880584716797\n",
      "step=(0, 8) data=train_loss: 64.65880584716797\n",
      "step=(0, 9) data='train_loss':61.79557800292969\n",
      "step=(0, 9) data=train_loss: 61.79557800292969\n",
      "step=(0, 10) data='train_loss':69.2992935180664\n",
      "step=(0, 10) data=train_loss: 69.2992935180664\n",
      "step=(0, 11) data='train_loss':40.36468505859375\n",
      "step=(0, 11) data=train_loss: 40.36468505859375\n",
      "step=(0, 12) data='train_loss':40.047733306884766\n",
      "step=(0, 12) data=train_loss: 40.047733306884766\n",
      "step=(0, 13) data='train_loss':50.25033187866211\n",
      "step=(0, 13) data=train_loss: 50.25033187866211\n",
      "step=(0, 14) data='train_loss':37.44331359863281\n",
      "step=(0, 14) data=train_loss: 37.44331359863281\n",
      "step=(0, 15) data='train_loss':43.36185073852539\n",
      "step=(0, 15) data=train_loss: 43.36185073852539\n",
      "step=(0, 16) data='train_loss':41.581153869628906\n",
      "step=(0, 16) data=train_loss: 41.581153869628906\n",
      "step=(0, 17) data='train_loss':32.146080017089844\n",
      "step=(0, 17) data=train_loss: 32.146080017089844\n",
      "step=(0, 18) data='train_loss':30.41472625732422\n",
      "step=(0, 18) data=train_loss: 30.41472625732422\n",
      "step=(0, 19) data='train_loss':26.236461639404297\n",
      "step=(0, 19) data=train_loss: 26.236461639404297\n",
      "step=(0, 20) data='train_loss':27.053926467895508\n",
      "step=(0, 20) data=train_loss: 27.053926467895508\n",
      "step=(0, 21) data='train_loss':24.308210372924805\n",
      "step=(0, 21) data=train_loss: 24.308210372924805\n",
      "step=(0, 22) data='train_loss':26.62696075439453\n",
      "step=(0, 22) data=train_loss: 26.62696075439453\n",
      "step=(0, 23) data='train_loss':25.701757431030273\n",
      "step=(0, 23) data=train_loss: 25.701757431030273\n",
      "step=(0, 24) data='train_loss':24.583425521850586\n",
      "step=(0, 24) data=train_loss: 24.583425521850586\n",
      "step=(0, 25) data='train_loss':18.60487937927246\n",
      "step=(0, 25) data=train_loss: 18.60487937927246\n",
      "step=(0, 26) data='train_loss':20.652456283569336\n",
      "step=(0, 26) data=train_loss: 20.652456283569336\n",
      "step=(0, 27) data='train_loss':17.69133758544922\n",
      "step=(0, 27) data=train_loss: 17.69133758544922\n",
      "step=(0, 28) data='train_loss':19.65291976928711\n",
      "step=(0, 28) data=train_loss: 19.65291976928711\n",
      "step=(0, 29) data='train_loss':18.011844635009766\n",
      "step=(0, 29) data=train_loss: 18.011844635009766\n",
      "step=(0, 30) data='train_loss':14.443350791931152\n",
      "step=(0, 30) data=train_loss: 14.443350791931152\n",
      "step=(0, 31) data='train_loss':15.287418365478516\n",
      "step=(0, 31) data=train_loss: 15.287418365478516\n",
      "step=(0, 32) data='train_loss':12.61448860168457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m     52\u001b[0m         model\u001b[38;5;241m.\u001b[39mparameters(), args\u001b[38;5;241m.\u001b[39mgrad_clip_thresh)\n\u001b[1;32m     53\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_hw4/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_hw4/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.inference import prepare_input_sequence\n",
    "from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2 import inference\n",
    "from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.waveglow.model import WaveGlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHI I LOVE VIDEO GAMES.\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m----> 2\u001b[0m sequences, lengths \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_input_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     mel, _, _ \u001b[38;5;241m=\u001b[39m inference(sequences, lengths)\n",
      "File \u001b[0;32m~/Desktop/Spring 2024/DL/emotion_tts/DeepLearningExamples/PyTorch/SpeechSynthesis/Tacotron2/inference.py:166\u001b[0m, in \u001b[0;36mprepare_input_sequence\u001b[0;34m(texts, cpu_run)\u001b[0m\n\u001b[1;32m    164\u001b[0m text_padded, input_lengths \u001b[38;5;241m=\u001b[39m pad_sequences(d)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu_run:\n\u001b[0;32m--> 166\u001b[0m     text_padded \u001b[38;5;241m=\u001b[39m \u001b[43mtext_padded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    167\u001b[0m     input_lengths \u001b[38;5;241m=\u001b[39m input_lengths\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_hw4/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "text = \"HI I LOVE VIDEO GAMES.\" \n",
    "sequences, lengths = prepare_input_sequence([text])\n",
    "\n",
    "with torch.no_grad():\n",
    "    mel, _, _ = inference(sequences, lengths)\n",
    "    audio = WaveGlow.infer(mel)\n",
    "audio_numpy = audio[0].data.cpu().numpy()\n",
    "rate = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwavfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m write\n\u001b[0;32m----> 2\u001b[0m write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrate\u001b[49m, audio_numpy)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m      4\u001b[0m Audio(audio_numpy, rate\u001b[38;5;241m=\u001b[39mrate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rate' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.io.wavfile import write\n",
    "write(\"audio.wav\", rate, audio_numpy)\n",
    "from IPython.display import Audio\n",
    "Audio(audio_numpy, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDeepLearningExamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPyTorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSpeechSynthesis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTacotron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwaveglow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaveGlow\n\u001b[1;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHI I LOVE VIDEO GAMES.\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m----> 6\u001b[0m sequences, lengths \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_input_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     mel, _, _ \u001b[38;5;241m=\u001b[39m inference(sequences, lengths)\n",
      "File \u001b[0;32m~/Desktop/Spring 2024/DL/emotion_tts/DeepLearningExamples/PyTorch/SpeechSynthesis/Tacotron2/inference.py:166\u001b[0m, in \u001b[0;36mprepare_input_sequence\u001b[0;34m(texts, cpu_run)\u001b[0m\n\u001b[1;32m    164\u001b[0m text_padded, input_lengths \u001b[38;5;241m=\u001b[39m pad_sequences(d)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu_run:\n\u001b[0;32m--> 166\u001b[0m     text_padded \u001b[38;5;241m=\u001b[39m \u001b[43mtext_padded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    167\u001b[0m     input_lengths \u001b[38;5;241m=\u001b[39m input_lengths\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_hw4/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.inference import prepare_input_sequence\n",
    "from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2 import inference\n",
    "from DeepLearningExamples.PyTorch.SpeechSynthesis.Tacotron2.waveglow.model import WaveGlow\n",
    "\n",
    "text = \"HI I LOVE VIDEO GAMES.\" \n",
    "sequences, lengths = prepare_input_sequence([text])\n",
    "\n",
    "with torch.no_grad():\n",
    "    mel, _, _ = inference(sequences, lengths)\n",
    "    audio = WaveGlow.infer(mel)\n",
    "audio_numpy = audio[0].data.cpu().numpy()\n",
    "rate = 22050\n",
    "\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "write(\"audio.wav\", rate, audio_numpy)\n",
    "from IPython.display import Audio\n",
    "Audio(audio_numpy, rate=rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
