{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dlproj (Python 3.11.8)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dlproj ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from data_loader import get_train_loader\n",
    "from speechbrain.inference.vocoders import HIFIGAN\n",
    "from transformers import AutoModelForAudioClassification\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTextEmotionModel(nn.Module):\n",
    "    def __init__(self, num_emotions, embedding_dim, num_frames):\n",
    "        super(AudioTextEmotionModel, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # Emotion embedding layer\n",
    "        self.emotion_embedding = nn.Embedding(num_emotions, embedding_dim)\n",
    "\n",
    "        # Convolutional layers for mel spectrogram input\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(10, 5, kernel_size=(5, 5), padding=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(5, 1, kernel_size=(3, 3), padding=(1, 1))            \n",
    "        \n",
    "    def forward(self, audio_input, emotion_idx):\n",
    "        emotion_embedding = self.emotion_embedding(emotion_idx)\n",
    "        emotion_repeated = emotion_embedding.repeat(1, self.num_frames)\n",
    "\n",
    "        mel_emotion = torch.stack((audio_input, emotion_repeated), dim=0)\n",
    "\n",
    "        x = F.relu(self.conv1(mel_emotion))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        output = F.relu(self.conv3(x))\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=\"pretrained_models/tts-hifigan-ljspeech\")\n",
    "emotion_rec = AutoModelForAudioClassification.from_pretrained(\"3loi/SER-Odyssey-Baseline-WavLM-Categorical-Attributes\", trust_remote_code=True)\n",
    "mean = emotion_rec.config.mean\n",
    "std = emotion_rec.config.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = 8\n",
    "embedding_dim = 80\n",
    "num_frames = 444 \n",
    "lr = 0.001\n",
    "\n",
    "model = AudioTextEmotionModel(num_emotions, embedding_dim, num_frames)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for audio_input, emotion_input, text_input in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(audio_input, emotion_input)\n",
    "\n",
    "        waveforms = hifi_gan.decode_batch(output)\n",
    "\n",
    "        norm_wav = (waveforms - mean) / (std+0.000001)\n",
    "        mask = torch.ones(1, len(norm_wav))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = emotion_rec(norm_wav, mask)\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(pred, dim=1)\n",
    "\n",
    "        emotion_loss = emotion_criterion(output, emotion_input)\n",
    "        loss = emotion_loss \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg. Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss,\n",
    "            }, 'checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
